{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OUEr8ztPfwz",
    "outputId": "97b93d3b-d54c-4434-f235-b74b941b30a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
      "\u001b[K     |████████████████████████████████| 204.2MB 63kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 45.0MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=f8d393567a7c23f14f242db1330833f7f206108bf11f5e35970326f1d1dab072\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5gFSCm5PPf96"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "U3RqI5W9PwEU"
   },
   "outputs": [],
   "source": [
    "# creating spark session\n",
    "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-ynmbdljP37e"
   },
   "outputs": [],
   "source": [
    "# creating spark dataframe wiht the input data. You can also read the data from file. label represents the 3 documnets (0.0,0.1,0.2)\n",
    "sentenceData = spark.createDataFrame([\n",
    "        (0.0, \"Welcome to KDM TF_IDF Tutorial.\"),\n",
    "        (0.1, \"Learn Spark ml tf_idf in today's lab.\"),\n",
    "        (0.2, \"Spark Mllib has TF-IDF.\")\n",
    "    ], [\"label\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "VAuDw1JTP_j9"
   },
   "outputs": [],
   "source": [
    "# creating tokens/words from the sentence data\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juPnzf1HkBNi",
    "outputId": "dff68967-8f87-46e8-9dce-d881f0f35495"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Welcome to KDM TF...|[welcome, to, kdm...|\n",
      "|  0.1|Learn Spark ml tf...|[learn, spark, ml...|\n",
      "|  0.2|Spark Mllib has T...|[spark, mllib, ha...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vWWoLXIKQEp-"
   },
   "outputs": [],
   "source": [
    "# applying tf on the words data\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dqsyc3TFQLX4"
   },
   "outputs": [],
   "source": [
    "# calculating the IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ej95DLvjQS1T",
    "outputId": "70d9b478-0e5d-4a61-e662-cbd1a7e8c565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[2,8,13,15,17...|\n",
      "|  0.1|(20,[2,3,6,7],[0....|\n",
      "|  0.2|(20,[6,14,15],[0....|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#displaying the results\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UFY0atcwQiCs"
   },
   "outputs": [],
   "source": [
    "spark2 = SparkSession.builder.appName(\"Ngram Example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5j1F-kR6Q2Ay"
   },
   "outputs": [],
   "source": [
    "#creating dataframe of input\n",
    "wordDataFrame = spark2.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"id\", \"words\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8Xnz2_iOQ_xO"
   },
   "outputs": [],
   "source": [
    "#creating NGrams with n=2 (two words)\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMhJK57bRGD5",
    "outputId": "68f54f73-9b14-4e26-ee2e-dafce9448da6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|ngrams                                                            |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the results\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kzQ7KVW3Riz0"
   },
   "outputs": [],
   "source": [
    "# creating spark session\n",
    "spark3 = SparkSession.builder.appName(\"Word2Vec Example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gzHNuEa5Ri9E"
   },
   "outputs": [],
   "source": [
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark3.createDataFrame([\n",
    "    (\"McCarthy was asked to analyse the data from the first phase of trials of the vaccine.\".split(\" \"), ),\n",
    "    (\"We have amassed the raw data and are about to begin analysing it.\".split(\" \"), ),\n",
    "    (\"Without more data we cannot make a meaningful comparison of the two systems.\".split(\" \"), ),\n",
    "    (\"Collecting data is a painfully slow process.\".split(\" \"), ),\n",
    "    (\"You need a long series of data to be able to discern such a trend.\".split(\" \"), )\n",
    "], [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7IjCO6rRRjCu"
   },
   "outputs": [],
   "source": [
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmVIfXsHSHUs",
    "outputId": "af3ca5a8-0977-4837-fce2-cdde2b1bec10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [McCarthy, was, asked, to, analyse, the, data, from, the, first, phase, of, trials, of, the, vaccine.] => \n",
      "Vector: [0.022913106746273115,0.007734941667877138,-0.002375059062615037]\n",
      "\n",
      "Text: [We, have, amassed, the, raw, data, and, are, about, to, begin, analysing, it.] => \n",
      "Vector: [-0.04099500154216702,0.0049125014159541866,-0.0014882660112701931]\n",
      "\n",
      "Text: [Without, more, data, we, cannot, make, a, meaningful, comparison, of, the, two, systems.] => \n",
      "Vector: [0.022649812046438456,0.0013891176248972234,0.01336516855427852]\n",
      "\n",
      "Text: [Collecting, data, is, a, painfully, slow, process.] => \n",
      "Vector: [-0.052166125604084554,0.04153742528121386,-0.018508063097085272]\n",
      "\n",
      "Text: [You, need, a, long, series, of, data, to, be, able, to, discern, such, a, trend.] => \n",
      "Vector: [0.011940793506801129,0.020289071649312974,-0.0013307716076572736]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    #printing the results\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xCbv_TB_SeqZ",
    "outputId": "daa0e433-a17e-4af7-ef6c-12bb76bb14bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      word|        similarity|\n",
      "+----------+------------------+\n",
      "|      long|0.9953629374504089|\n",
      "|    cannot|0.9902032613754272|\n",
      "|meaningful|0.9105675220489502|\n",
      "|      from|0.8469968438148499|\n",
      "|     asked| 0.794264554977417|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing the synonyms and cosine similarity of the word in input data\n",
    "synonyms = model.findSynonyms(\"data\", 5)   # its okay for certain words , real bad for others\n",
    "synonyms.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CtdspYI3Qg1o"
   },
   "outputs": [],
   "source": [
    "#closing the spark sessions\n",
    "spark.stop()\n",
    "spark2.stop()\n",
    "spark3.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "KDM_ICP5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
